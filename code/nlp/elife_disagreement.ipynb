{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tolabel.csv\", sep=\"|\")\n",
    "df = df[[\"Manuscript no.\", \"Reviewer ID\", \"CleanedComments\", \"Rec\", \"Suitable\", \"ShouldBe\", \"HumanLabel\"]]\n",
    "df = df.set_index([\"Manuscript no.\"])\n",
    "scored_bert = pd.read_csv(\"metascience/PeerRead/bert_output_20191104/eval_results_full_allelife.txt\", \n",
    "                          sep=\"\\t\", names=[\"id\", \"score\", \"dummy\", \"text\"])\n",
    "\n",
    "list(scored_bert.sort_values(by=\"score\", ascending=False).iloc[1:10,][\"text\"])\n",
    "df[\"score\"] = list(scored_bert.score)\n",
    "df[\"Text\"] = list(scored_bert.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewers = pd.read_csv(\"gender_reviewers.csv\", error_bad_lines=False)\n",
    "# this is wrong\n",
    "reviewers_data = pd.DataFrame(reviewers.groupby(\"Reviewer ID\")[\"Reviewer name\"].count())\n",
    "reviewers_data.columns = [\"reviewer_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewers[\"review_count\"] = reviewers.groupby(\"Reviewer ID\")[\"gender\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = []\n",
    "for i in reviewers[\"Reviewer email\"].str.split(\".\"):\n",
    "    try:\n",
    "        domain += [i[-1]]\n",
    "    except TypeError:\n",
    "        domain += [\"\"]\n",
    "reviewers[\"domain\"] = domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in paper history stuff\n",
    "e = pd.read_csv(\"/share/pi/dmcfarla/eLifeRawData/DM_Data_Reviews/DM_Data/eLife_Paper_history_2019_03_15.csv\")\n",
    "e[\"Manuscript no.\"] = e[\"ms\"]\n",
    "e = e.set_index([\"Manuscript no.\"])\n",
    "e = e.dropna(subset=[\"full_decision\"])\n",
    "\n",
    "# to get finaldecision, take last non-NA decision of the ones listed here\n",
    "# note that this excludes rejected by initial decision\n",
    "e[\"FinalDecision\"] = e.apply(lambda x: list(x[[\"full_decision\", \"rev1_decision\", \"rev2_decision\", \"rev3_decision\", \"rev4_decision\"]].dropna())[-1], axis=1)\n",
    "e[\"outcome\"] = np.where(e[\"FinalDecision\"] == \"Accept Full Submission\", 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = df.join(e)\n",
    "#df_e = df_e.set_index([\"ms\"])\n",
    "#df_e = df.reset_index()\n",
    "#df_e = df_e.merge(reviewers, on=[\"Reviewer ID\"])\n",
    "#df_e = df_e.set_index(df_e[\"Manuscript no.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.groupby([\"outcome\"]).mean()[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.DataFrame(df_e.groupby([\"country\"]).count()[\"Rec\"])\n",
    "countries[\"mean_score\"] = df_e.groupby([\"country\"]).mean()[\"score\"]\n",
    "countries[\"mean_outcome\"] = df_e.groupby([\"country\"]).mean()[\"outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.loc[countries[\"Rec\"] > 300, \"mean_score\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.loc[countries[\"Rec\"] > 300, \"mean_outcome\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = df.loc[~pd.isna(df.HumanLabel)]\n",
    "labeled = labeled.loc[labeled.HumanLabel <= 5]\n",
    "labeled[[\"score\", \"HumanLabel\"]].corr()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(labeled.HumanLabel, labeled.score)\n",
    "labeled.HumanLabel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(labeled.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(labeled.HumanLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e[\"review_outcome\"] = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e[\"zscore\"] = (df_e.score - np.mean(df_e.score))/np.std(df_e.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.loc[(df_e.zscore > 1) & (df_e.outcome == 1), \"review_outcome\" ] = \"pos_pos\"\n",
    "df_e.loc[(df_e.zscore > 1) & (df_e.outcome == 0), \"review_outcome\" ] = \"pos_neg\"\n",
    "df_e.loc[(df_e.zscore < -1) & (df_e.outcome == 0), \"review_outcome\" ] = \"neg_neg\"\n",
    "df_e.loc[(df_e.zscore < -1) & (df_e.outcome == 1), \"review_outcome\" ] = \"neg_pos\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.review_outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers with disagreement\n",
    "disagreement = df_e.loc[(df_e.review_outcome == \"pos_neg\") | (df_e.review_outcome == \"neg_pos\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_papers = df_e.loc[set(disagreement.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_disagreement():\n",
    "    ex = disagreement_papers.loc[random.choice(disagreement_papers.index)]\n",
    "    ex = (ex[[\"CleanedComments\", \"score\", \"outcome\"]])\n",
    "    print(\"outcome:\", list(ex[\"outcome\"])[0])\n",
    "    for i in range(ex.shape[0]):\n",
    "        print(ex.iloc[i][\"score\"], ex.iloc[i][\"CleanedComments\"] + \"\\n\")\n",
    "    \n",
    "get_example_disagreement()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_papers[\"winner\"] = np.where(list((disagreement_papers.review_outcome == \"neg_neg\") | (disagreement_papers.review_outcome == \"pos_pos\")), \"winner\", \"none\")\n",
    "disagreement_papers[\"winner\"] = np.where(list((disagreement_papers.review_outcome == \"neg_pos\") | (disagreement_papers.review_outcome == \"pos_neg\")), \"loser\", disagreement_papers.winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_papers.review_outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_papers[\"length_text\"] = (disagreement_papers[\"CleanedComments\"].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_papers.groupby(\"winner\").mean()[\"length_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners_losers = disagreement_papers.loc[(disagreement_papers.winner == \"winner\") | (disagreement_papers.winner == \"loser\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a classifier that predicts outcome based on review\n",
    "x = winners_losers\n",
    "x.initial_qc_dt = pd.to_datetime(x.initial_qc_dt)\n",
    "train = x[pd.to_datetime(x.initial_qc_dt) <= pd.to_datetime(\"2017-06-30\")]\n",
    "test = x[((x.initial_qc_dt > pd.to_datetime(\"2017-06-30\")) & (x.initial_qc_dt < pd.to_datetime(\"2018-01-01\")))]\n",
    "word_vectorizer = CountVectorizer(analyzer='word')\n",
    "tags = [i == \"winner\" for i in train[\"winner\"]]\n",
    "test_tags = [i == \"winner\" for i in test[\"winner\"]]\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=20, max_df=0.8, ngram_range=(1, 2),\n",
    "                             stop_words=stopwords.words('english'))\n",
    "processed_features = vectorizer.fit_transform(list(train[\"CleanedComments\"]))\n",
    "test_processed_features = vectorizer.transform(list(test[\"CleanedComments\"]))\n",
    "\n",
    "text_classifier = LogisticRegression()\n",
    "text_classifier.fit(processed_features, tags)\n",
    "predictions = text_classifier.predict(processed_features)\n",
    "\n",
    "# in-sample\n",
    "np.mean(predictions == tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top10 features, bottom10\n",
    "def print_top10(vectorizer, clf):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print (\"top 10\")\n",
    "    for j in reversed(np.argsort(clf.coef_[0])[-10:]):\n",
    "        print (feature_names[j])\n",
    "    print (\"\\nbottom 10\")\n",
    "    for j in np.argsort(clf.coef_[0])[0:10]:\n",
    "        print (feature_names[j])\n",
    "\n",
    "print_top10(vectorizer, text_classifier)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_papers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dis = disagreement_papers.reset_index().merge(reviewers, on=[\"Manuscript no.\", \"Reviewer ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dis.groupby(\"winner\").mean()[\"review_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dis[\"gender_binary\"] = review_dis[\"gender\"].str.contains(\"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dis.groupby(\"winner\").mean()[\"gender_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dis.gender_binary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = review_dis.groupby([\"domain\", \"winner\"]).count()[\"Manuscript no.\"]\n",
    "sums = c.groupby(level=0).sum().reset_index()\n",
    "keepers = sums.loc[sums[\"Manuscript no.\"] > 100][\"domain\"]\n",
    "c = c.loc[keepers]\n",
    "sums = c.groupby(level=0).sum()\n",
    "review_domain_winners = c/sums\n",
    "review_domain_winners = review_domain_winners.reset_index()\n",
    "review_domain_winners.loc[review_domain_winners.winner == \"winner\"].sort_values(\"Manuscript no.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}